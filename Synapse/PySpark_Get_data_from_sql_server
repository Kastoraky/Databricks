# Import the Python libraries
from pyspark.sql import *
import pandas as pd

# COMMAND ----------
#Create variables for the connection
jdbcHostname = "mypersonaldatabaseserver.database.windows.net"
jdbcDatabase = "Dictionary"
jdbcPort = 1433
jdbcUsername = "Kastoraky"
jdbcPassword = dbutils.secrets.get(scope="AzureSqlPassword", key="AzureSqlPassword")
jdbcUrl = "jdbc:sqlserver://{0}:{1};database={2};user={3};password={4}".format(jdbcHostname, jdbcPort, jdbcDatabase, jdbcUsername, jdbcPassword)

# COMMAND ----------
# Make the actual connection

jdbcUrl = "jdbc:sqlserver://{0}:{1};database={2}".format(jdbcHostname, jdbcPort, jdbcDatabase)
connectionProperties = {
  "user" : jdbcUsername,
  "password" : jdbcPassword,
  "driver" : "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

# COMMAND ----------
# Declare the table from SQL Server that wants to be used in the notebook with %sql

empl = "(SELECT TOP (1000) * FROM [dbo].[Employee]) emp_alias"
df_empl = spark.read.jdbc(url=jdbcUrl, table=empl, properties=connectionProperties)
df_empl.createOrReplaceTempView("source") 
